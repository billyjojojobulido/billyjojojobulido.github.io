<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Reinforcement Learning (RL) | Nir Lipovetzky</title>
    <link>https://nirlipo.github.io/tag/deep-reinforcement-learning-rl/</link>
      <atom:link href="https://nirlipo.github.io/tag/deep-reinforcement-learning-rl/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Reinforcement Learning (RL)</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 02 Jun 2015 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nirlipo.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Reinforcement Learning (RL)</title>
      <link>https://nirlipo.github.io/tag/deep-reinforcement-learning-rl/</link>
    </image>
    
    <item>
      <title>Arcade Learning Environment</title>
      <link>https://nirlipo.github.io/project/atari/</link>
      <pubDate>Tue, 02 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://nirlipo.github.io/project/atari/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/P-603qPMkSg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The Atari 2600 games supported in the Arcade Learning Environment 
&lt;a href=&#34;http://www.arcadelearningenvironment.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(ALE)&lt;/a&gt; all feature a known initial (RAM) state and actions that have deterministic effects. Classical planners, however, cannot be used off-the-shelf as there is no compact PDDL-model of the games, and action effects and goals are not known a priori. Indeed, there are no explicit goals, and the planner must select actions on-line while interacting with a simulator that returns successor states and rewards. None of this precludes the use of blind lookahead algorithms for action selection like breadth-first search or Dijkstraâ€™s yet such methods are not effective over large state spaces. We thus turn to a different class of planning methods introduced recently that have been shown to be effective for solving large planning problems but which do not require prior knowledge of state transitions, costs (rewards) or goals. The empirical results over 54 Atari games show that the simplest such algorithm performs at the level of UCT, the state-of-the-art planning method in this domain, and suggest the potential of width-based methods for planning with simulators when factored, compact action models are not available.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
